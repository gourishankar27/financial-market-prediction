{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "219d4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fredapi import Fred\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3df336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# START_DATE = '2000-01-01'\n",
    "START_DATE = '1990-01-01'  # earliest VIX data: gain ~2000 observations\n",
    "# END_DATE = '2024-09-01'\n",
    "\n",
    "# FRED series mapping\n",
    "FRED_SERIES = {\n",
    "    'VIX': 'VIXCLS',                    # VIX Volatility Index\n",
    "    'Treasury_10Y': 'DGS10',            # 10-Year Treasury Rate\n",
    "    'Yield_Spread': 'T10Y2Y',           # 10Y-2Y Yield Spread (daily, FRED-calculated, no lag adjustment needed)\n",
    "    'CPI': 'CPIAUCSL',                  # Consumer Price Index\n",
    "    'Unemployment': 'UNRATE',           # Unemployment Rate\n",
    "    'Fed_Rate': 'FEDFUNDS',             # Federal Funds Rate\n",
    "    'Consumer_Sentiment': 'UMCSENT',    # Consumer Sentiment Index\n",
    "    # 'Credit_HY': 'BAMLH0A0HYM2',        # High yield credit spread\n",
    "    # 'Credit_IG': 'BAMLC0A0CM'           # Investment grade credit spread\n",
    "    # 'TB3MS': 'TB3MS',                   # for 10Y-3M yield spread - redundant with 10-2Y; removed\n",
    "    # 'PCE': 'PCEPI',                     # alternative inflation measure - highly correlated with CPI; removed\n",
    "    # 'GDP': 'GDP',                       # problematic. quarterly updates. vintage alignment complex; removed\n",
    "    'Industrial_Production': 'INDPRO',  # Industrial Production Index\n",
    "}\n",
    "\n",
    "# Yahoo Finance tickers mapping\n",
    "YAHOO_TICKERS = {\n",
    "    # Volatility measures\n",
    "    # 'VVIX': '^VVIX',                    # Volatility of VIX\n",
    "    \n",
    "    # Sector ETFs (Select Sector SPDRs)\n",
    "    # 'Sector_Tech': 'XLK',               # Technology\n",
    "    # 'Sector_Financials': 'XLF',         # Financials\n",
    "    # 'Sector_Energy': 'XLE',             # Energy\n",
    "    # 'Sector_Healthcare': 'XLV',         # Healthcare\n",
    "    # 'Sector_ConsumerDiscretionary': 'XLY',  # Consumer Discretionary\n",
    "    # 'Sector_ConsumerStaples': 'XLP',    # Consumer Staples\n",
    "    # 'Sector_Industrials': 'XLI',        # Industrials\n",
    "    # 'Sector_Materials': 'XLB',          # Materials\n",
    "    # 'Sector_Utilities': 'XLU',          # Utilities\n",
    "    # 'Sector_RealEstate': 'XLRE',        # Real Estate: began in 2015\n",
    "    # 'Sector_Communications': 'XLC',     # Communication Services: began in 2018\n",
    "    \n",
    "    # Broad market measures\n",
    "    'Wilshire5000': '^W5000',           # Wilshire 5000 Total Market Index\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419dce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRED_API_KEY = \"c8d5b4c26407e7cbfcecca702e0e7aee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2328faa-53e6-4797-8424-b7efac43c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release date alignment method\n",
    "USE_VINTAGE_DATES = False    # set True to use ALFRED, False for fixed shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726ab9a9-6465-4621-a7af-c45835444f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_section(title, char='='):\n",
    "    \"\"\"print a formatted section header\"\"\"\n",
    "    print(f'\\n{char * 70}')\n",
    "    print(f'{title}')\n",
    "    print(f'{char * 70}\\n')\n",
    "\n",
    "def print_subsection(title):\n",
    "    \"\"\"print a formatted subsection header\"\"\"\n",
    "    print(f'\\n{title}')\n",
    "    print(f'{\"-\" * 70}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab73eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_market_data(start_date=START_DATE,):\n",
    "    \"\"\"Collect S&P 500 market data from Yahoo Finance\"\"\"\n",
    "    print_subsection('Collecting S&P 500 Market Data')\n",
    "\n",
    "    # Download S&P 500 data\n",
    "    sp500 = yf.download(\"^GSPC\", start=start_date,  progress=False)\n",
    "\n",
    "    # Handle MultiIndex columns if present\n",
    "    if hasattr(sp500.columns, 'nlevels') and sp500.columns.nlevels > 1:\n",
    "        sp500.columns = [col[0] for col in sp500.columns]\n",
    "\n",
    "    # Calculate daily returns (percentage)\n",
    "    sp500['Returns'] = sp500['Close'].pct_change() * 100\n",
    "\n",
    "    # Keep only essential columns\n",
    "    market_data = sp500[['Close', 'Volume', 'Returns']].copy()\n",
    "    market_data.columns = ['SP500_Close', 'SP500_Volume', 'SP500_Returns']\n",
    "\n",
    "    print(f'   Collected {len(market_data)} days of S&P 500 data')\n",
    "    print(f'   Date range: {market_data.index[0]} to {market_data.index[-1]}')\n",
    "    return market_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a8b54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fred_data(api_key, start_date=START_DATE):\n",
    "    \"\"\"Collect macroeconomic data from FRED\"\"\"\n",
    "    print_subsection('Collecting FRED Macroeconomic Data')\n",
    "\n",
    "    fred = Fred(api_key=api_key)\n",
    "    fred_data = {}\n",
    "\n",
    "    for name, series_id in FRED_SERIES.items():\n",
    "        try:\n",
    "            data = fred.get_series(series_id, start=start_date)\n",
    "            fred_data[name] = data\n",
    "            print(f\"   ✓ {name}: {len(data)} observations\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ {name}: Error - {str(e)}\")\n",
    "            fred_data[name] = None\n",
    "\n",
    "    print(f'\\n   Summary: Successfully collected {sum(1 for v in fred_data.values() if v is not None)}/{len(FRED_SERIES)} series')\n",
    "    return fred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee7ec1b-2da2-4bd4-b755-0c2d82580cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_yahoo_data(tickers, start_date=START_DATE):\n",
    "    \"\"\"\n",
    "    collect data from Yahoo finance for multiple tickers\n",
    "    \n",
    "    params\n",
    "    -----------\n",
    "    tickers : dict\n",
    "        dict mapping feature names toy ahoo ticker symbols\n",
    "        example: {'VVIX': '^VVIX', 'Tech_Sector': 'XLK'}\n",
    "    start_date : str\n",
    "        start date for data collection\n",
    "        \n",
    "    retruns\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        dataframe with all yahoo-sourced features, indexed by date\n",
    "    \"\"\"\n",
    "    print_subsection('Collecting Yahoo Finance Data')\n",
    "    \n",
    "    yahoo_data = {}\n",
    "    \n",
    "    for name, ticker in tickers.items():\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, progress=False)\n",
    "            \n",
    "            # Handle MultiIndex columns if present\n",
    "            if hasattr(data.columns, 'nlevels') and data.columns.nlevels > 1:\n",
    "                data.columns = [col[0] for col in data.columns]\n",
    "            \n",
    "            # Keep only Close price (can add Volume later if needed)\n",
    "            yahoo_data[name] = data['Close']\n",
    "            print(f\"   ✓ {name} ({ticker}): {len(data)} observations\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ {name} ({ticker}): Error - {str(e)}\")\n",
    "            yahoo_data[name] = None\n",
    "    \n",
    "    # Combine into single DataFrame, remove None entries\n",
    "    yahoo_data = {k: v for k, v in yahoo_data.items() if v is not None}\n",
    "    combined = pd.DataFrame(yahoo_data) if yahoo_data else pd.DataFrame()\n",
    "    \n",
    "    print(f'\\n   Summary: Successfully collected {len(yahoo_data)}/{len(tickers)} tickers')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6c3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_combine_data(market_data, fred_data, yahoo_data):\n",
    "    \"\"\"Align different frequency data and combine into master dataset\"\"\"\n",
    "    print(\"\\nAligning and combining data...\")\n",
    "\n",
    "    # Start with market data (daily frequency)\n",
    "    master_df = market_data.copy()\n",
    "\n",
    "    # Add FRED data (forward-fill for non-trading days)\n",
    "    for name, series in fred_data.items():\n",
    "        if series is not None:\n",
    "            # Reindex to match market data dates and forward-fill\n",
    "            aligned_series = series.reindex(master_df.index, method='ffill')\n",
    "            master_df[name] = aligned_series\n",
    "\n",
    "    # add yahoo data if provided\n",
    "    if yahoo_data is not None and not yahoo_data.empty:\n",
    "        for col in yahoo_data.columns:\n",
    "            # Reindex to match market data dates and forward-fill\n",
    "            aligned_series = yahoo_data[col].reindex(master_df.index, method='ffill')\n",
    "            master_df[col] = aligned_series\n",
    "\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db89a9a-7e4c-4d0e-b8e3-73c1d9b8d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    '''\n",
    "    Perform comprehensive data quality checks.\n",
    "    '''\n",
    "    print_subsection('Data Quality Check')\n",
    "    \n",
    "    # Check missing values\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    print(f'Total missing values: {total_missing}')\n",
    "    if total_missing > 0:\n",
    "        print('\\nMissing by column:')\n",
    "        missing_cols = df.isnull().sum()\n",
    "        for col, count in missing_cols[missing_cols > 0].items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f'  {col}: {count} ({pct:.1f}%)')\n",
    "    \n",
    "    # Check for extreme returns (potential data errors)\n",
    "    if 'SP500_Returns' in df.columns:\n",
    "        extreme_returns = df[abs(df['SP500_Returns']) > 10]\n",
    "        print(f'\\nExtreme daily returns (>10%): {len(extreme_returns)}')\n",
    "        if len(extreme_returns) > 0 and len(extreme_returns) < 10:\n",
    "            print('Dates with extreme returns:')\n",
    "            for date, ret in extreme_returns['SP500_Returns'].items():\n",
    "                print(f'  {date.strftime(\"%Y-%m-%d\")}: {ret:.2f}%')\n",
    "    \n",
    "    # Check data types\n",
    "    print('\\nData types:')\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba4075d6-434e-492f-803f-2a4d56dfac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vintage_date_alignment(df, api_key):\n",
    "    '''\n",
    "    apply exact release date alignment using ALFRED vintage dates\n",
    "    \n",
    "    falls back to forward-fill for periods where vintage data unavailable\n",
    "    '''\n",
    "    print_subsection('Applying ALFRED vintage date alignment')\n",
    "    \n",
    "    fred = Fred(api_key=api_key)\n",
    "    df_aligned = df.copy()\n",
    "    \n",
    "    series_mapping = {\n",
    "        'CPI': 'CPIAUCSL',\n",
    "        # 'PCE': 'PCEPI',\n",
    "        # 'TB3MS': 'TB3MS',\n",
    "        'Unemployment': 'UNRATE',\n",
    "        'Industrial_Production': 'INDPRO',\n",
    "        'Fed_Rate': 'FEDFUNDS',\n",
    "        'Consumer_Sentiment': 'UMCSENT'\n",
    "    }\n",
    "    \n",
    "    alignment_report = []  # Track what happened to each series\n",
    "    \n",
    "    for col_name, series_id in series_mapping.items():\n",
    "        print(f'  Processing {col_name} ({series_id})...')\n",
    "        \n",
    "        try:\n",
    "            # Get all vintage releases\n",
    "            vintages = fred.get_series_all_releases(series_id)\n",
    "            vintages['date'] = pd.to_datetime(vintages['date'])\n",
    "            vintages['realtime_start'] = pd.to_datetime(vintages['realtime_start'])\n",
    "            \n",
    "            # Create aligned series\n",
    "            aligned_series = pd.Series(index=df_aligned.index, dtype=float)\n",
    "            \n",
    "            for idx in df_aligned.index:\n",
    "                available_vintages = vintages[vintages['realtime_start'] <= idx]\n",
    "                \n",
    "                if len(available_vintages) > 0:\n",
    "                    past_obs = available_vintages[available_vintages['date'] < idx]\n",
    "                    \n",
    "                    if len(past_obs) > 0:\n",
    "                        most_recent = past_obs.sort_values(['date', 'realtime_start'], ascending=False).iloc[0]\n",
    "                        aligned_series.loc[idx] = most_recent['value']\n",
    "                    else:\n",
    "                        aligned_series.loc[idx] = np.nan\n",
    "                else:\n",
    "                    aligned_series.loc[idx] = np.nan\n",
    "            \n",
    "            # Count vintage coverage\n",
    "            vintage_count = aligned_series.notna().sum()\n",
    "            total_count = len(df_aligned)\n",
    "            \n",
    "            # Store original for comparison\n",
    "            original_series = aligned_series.copy()\n",
    "            \n",
    "            # If vintage has NaNs, fall back to original forward-filled values\n",
    "            aligned_series = aligned_series.fillna(df_aligned[col_name])\n",
    "            \n",
    "            # Count how many were filled vs vintage\n",
    "            filled_count = aligned_series.notna().sum() - vintage_count\n",
    "            still_missing = aligned_series.isna().sum()\n",
    "            \n",
    "            # Replace column\n",
    "            df_aligned[col_name] = aligned_series\n",
    "            \n",
    "            # Build report\n",
    "            first_valid = aligned_series.first_valid_index()\n",
    "            report = {\n",
    "                'series': col_name,\n",
    "                'vintage_values': vintage_count,\n",
    "                'forward_filled': filled_count,\n",
    "                'still_missing': still_missing,\n",
    "                'first_valid_date': first_valid\n",
    "            }\n",
    "            alignment_report.append(report)\n",
    "            \n",
    "            # Print summary for this series\n",
    "            if filled_count > 0 or still_missing > 0:\n",
    "                print(f'    ✓ Aligned {col_name}: {vintage_count} vintage, {filled_count} forward-filled, {still_missing} missing')\n",
    "                if first_valid:\n",
    "                    print(f'      First valid date: {first_valid.strftime(\"%Y-%m-%d\")}')\n",
    "            else:\n",
    "                print(f'    ✓ Aligned {col_name} ({vintage_count}/{total_count} all from vintage)')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'    Error aligning {col_name}: {str(e)}')\n",
    "            print(f'    Keeping original values')\n",
    "            alignment_report.append({\n",
    "                'series': col_name,\n",
    "                'vintage_values': 0,\n",
    "                'forward_filled': 0,\n",
    "                'still_missing': len(df_aligned),\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Print final summary\n",
    "    print('\\n VINTAGE ALIGNMENT SUMMARY:')\n",
    "    print('─' * 70)\n",
    "    for report in alignment_report:\n",
    "        if 'error' in report:\n",
    "            print(f\"  {report['series']}: ERROR - {report['error']}\")\n",
    "        else:\n",
    "            total = len(df_aligned)\n",
    "            vintage_pct = (report['vintage_values'] / total * 100) if total > 0 else 0\n",
    "            filled_pct = (report['forward_filled'] / total * 100) if total > 0 else 0\n",
    "            print(f\"  {report['series']}:\")\n",
    "            print(f\"    - Vintage dates: {report['vintage_values']:,} ({vintage_pct:.1f}%)\")\n",
    "            if report['forward_filled'] > 0:\n",
    "                print(f\"    - Forward-filled: {report['forward_filled']:,} ({filled_pct:.1f}%)\")\n",
    "            if report['still_missing'] > 0:\n",
    "                print(f\"    - Still missing: {report['still_missing']:,}\")\n",
    "    \n",
    "    return df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f19aa1-47e0-402b-ae80-f0c55c135c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fixed_shift_alignment(df):\n",
    "    '''\n",
    "    apply fixed-day shift approximation for release dates.\n",
    "    '''\n",
    "    print_subsection('Applying fixed-shift release date alignment')\n",
    "    \n",
    "    # fix release date lags to prevent look-ahead bias\n",
    "    # shift forward = make data available LATER (i.e. when actually released)\n",
    "    df['CPI'] = df['CPI'].shift(14)  # released ~2 weeks after month end\n",
    "    # df['PCE'] = df['PCE'].shift(21)  # released ~3 weeks after month end\n",
    "    df['Unemployment'] = df['Unemployment'].shift(7)  # first fri of month\n",
    "    df['Industrial_Production'] = df['Industrial_Production'].shift(14)  # ~2 weeks\n",
    "    df['Fed_Rate'] = df['Fed_Rate'].shift(7)  # ~1 week after month end\n",
    "    df['Consumer_Sentiment'] = df['Consumer_Sentiment'].shift(2)  # final release ~month end    \n",
    "    # df['TB3MS'] = df['TB3MS'].shift(7)  # ~1 week after month end\n",
    "    print('   ✓ Release date adjustments applied')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c523805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "\n",
    "    # Print initial data quality\n",
    "    check_data_quality(df)\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CREDIT SPREAD MISSING DATA DIAGNOSTICS ===\")\n",
    "    if 'Credit_HY' in df.columns:\n",
    "        missing_hy = df['Credit_HY'].isnull()\n",
    "        missing_ig = df['Credit_IG'].isnull()\n",
    "        \n",
    "        print(f\"Credit_HY missing: {missing_hy.sum()}\")\n",
    "        print(f\"Credit_IG missing: {missing_ig.sum()}\")\n",
    "        \n",
    "        if missing_hy.sum() > 0:\n",
    "            print(f\"First missing date: {df[missing_hy].index.min()}\")\n",
    "            print(f\"Last missing date: {df[missing_hy].index.max()}\")\n",
    "            print(f\"First valid date: {df[~missing_hy].index.min()}\")\n",
    "            \n",
    "        # Check if missing values are contiguous (all at start/end)\n",
    "        first_valid_idx = df['Credit_HY'].first_valid_index()\n",
    "        last_valid_idx = df['Credit_HY'].last_valid_index()\n",
    "        print(f\"Credit spreads valid from {first_valid_idx} to {last_valid_idx}\")\n",
    "\n",
    "    print(\"\\n=== MISSING DATA PATTERN ===\")\n",
    "    # Check how missing values are distributed\n",
    "    missing_dates = df[df['Credit_HY'].isnull()].index\n",
    "    missing_by_year = missing_dates.year.value_counts().sort_index()\n",
    "    print(\"Missing Credit_HY observations by year:\")\n",
    "    print(missing_by_year)\n",
    "    \n",
    "    # Sample of dates with missing credit spreads\n",
    "    print(\"\\nSample of dates with missing credit spreads (first 20):\")\n",
    "    print(missing_dates[:20].tolist())\n",
    "    \n",
    "    # Check day of week pattern\n",
    "    print(\"\\nMissing by day of week:\")\n",
    "    print(missing_dates.dayofweek.value_counts().sort_index())\n",
    "    print(\"(0=Monday, 6=Sunday)\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward-fill macro variables (monthly data in daily frequency)\n",
    "    # market data (VIX, SP500, yields) should not have missing values\n",
    "    print_subsection('Forward-filling macro variables...')\n",
    "    macro_vars = ['CPI', 'Unemployment', 'Fed_Rate', \n",
    "                  'Consumer_Sentiment', 'Industrial_Production',\n",
    "                   ]\n",
    "    df_clean = df.copy()\n",
    "    df_clean[macro_vars] = df_clean[macro_vars].fillna(method='ffill')\n",
    "    \n",
    "    # verify no remaining NaNs in critical columns\n",
    "    remaining_na = df_clean[macro_vars].isnull().sum()\n",
    "    if remaining_na.sum() > 0:\n",
    "        print('   Warning: NaN values remain after forward-fill:')\n",
    "        print(remaining_na[remaining_na > 0])\n",
    "    else:\n",
    "        print('   ✓ All macro variables forward-filled successfully')\n",
    "\n",
    "    # apply release date alignment based on configuration\n",
    "    if USE_VINTAGE_DATES:\n",
    "        df_clean = apply_vintage_date_alignment(df_clean, FRED_API_KEY)\n",
    "    else:\n",
    "        df_clean = apply_fixed_shift_alignment(df_clean)\n",
    "    \n",
    "    # shifts will create NaNs that need to be removed\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "    if 'CPI' in df_clean.columns:\n",
    "        df_clean['Inflation_YoY'] = df_clean['CPI'].pct_change(252) * 100  # 252 trading days ≈ 1 year\n",
    "        # use 12 periods over 252 trading days to be more precise\n",
    "        # df_clean['Inflation_YoY'] = df_clean['CPI'].pct_change(12) * 100\n",
    "\n",
    "    # if 'PCE' in df_clean.columns:\n",
    "    #     df_clean['PCE_YoY'] = df_clean['PCE'].pct_change(12) * 100\n",
    "\n",
    "    if 'SP500_Close' in df_clean.columns:\n",
    "        df_clean['SP500_Volatility'] = df_clean['SP500_Returns'].rolling(20).std()\n",
    "\n",
    "    print(f\"   ✓ Final dataset shape: {df_clean.shape}\")\n",
    "    print(f\"   ✓ Date range: {df_clean.index[0]} to {df_clean.index[-1]}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70702fc-359f-4fe1-b1d5-76ea539a0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_frequency(df, freq='ME'):\n",
    "    '''\n",
    "    Resample daily data to weekly or monthly frequency.\n",
    "\n",
    "    - takes last value for price levels\n",
    "    - recalculates returns from resampled prices\n",
    "    - for monthly: applies 1-month lag to macro variables\n",
    "    '''\n",
    "    print_subsection(f'\\nResampling to {freq} frequency...')\n",
    "    \n",
    "    # Define aggregation rules\n",
    "    # Special case: volume gets averaged, everything else gets end-of-period\n",
    "    agg_rules = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'Volume' in col:\n",
    "            agg_rules[col] = 'mean'  # Average daily volume\n",
    "        else:\n",
    "            agg_rules[col] = 'last'  # End-of-period value for prices, rates, indices\n",
    "    \n",
    "    # Resample\n",
    "    resampled = df.resample(freq).agg(agg_rules)\n",
    "    \n",
    "    # Recalculate derived features from resampled data\n",
    "    if 'SP500_Close' in resampled.columns:\n",
    "        resampled['SP500_Returns'] = resampled['SP500_Close'].pct_change() * 100\n",
    "    if 'CPI' in resampled.columns:\n",
    "        resampled['Inflation_YoY'] = resampled['CPI'].pct_change(12) * 100\n",
    "    if 'SP500_Returns' in resampled.columns:\n",
    "        resampled['SP500_Volatility'] = resampled['SP500_Returns'].rolling(12).std()\n",
    "    \n",
    "    # For monthly frequency: apply additional 1-period lag to macro variables\n",
    "    if freq in ['M', 'ME', 'MS']:  # Month end, month start\n",
    "        macro_vars = ['CPI', 'Unemployment', 'Industrial_Production', \n",
    "                      'Fed_Rate', 'Consumer_Sentiment']\n",
    "        for var in macro_vars:\n",
    "            if var in resampled.columns:\n",
    "                resampled[var] = resampled[var].shift(1)\n",
    "    \n",
    "    # Drop NaNs from rolling windows and shifts\n",
    "    resampled = resampled.dropna()\n",
    "    \n",
    "    print(f'   ✓ Resampled shape: {resampled.shape}')\n",
    "    print(f'   ✓ Date range: {resampled.index[0]} to {resampled.index[-1]}')\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aed88dea-4771-4d85-b104-66d076c2e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(df, train_pct = 0.7, val_pct = 0.15):\n",
    "    '''\n",
    "    create temporal train/validation/test splits\n",
    "    \n",
    "    params:\n",
    "    -------\n",
    "    df : dataframe w/ DatetimeIndex\n",
    "    train_pct : float, proportion for training (default 0.7)\n",
    "    val_pct : float, proportion for validation (default 0.15)\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "    train, val, test : tuple of dataframes\n",
    "    \n",
    "    notes:\n",
    "    -------\n",
    "    uses temporal splits (not random) to avoid lookahead\n",
    "    test set is most recent data\n",
    "    '''\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_pct)\n",
    "    val_end = int(n * (train_pct + val_pct))\n",
    "    \n",
    "    train = df.iloc[:train_end].copy()\n",
    "    val = df.iloc[train_end:val_end].copy()\n",
    "    test = df.iloc[val_end:].copy()\n",
    "    \n",
    "    print('\\n DATA SPLITS:')\n",
    "    print('─' * 70)\n",
    "    print(f'Train: {train.index[0].strftime(\"%Y-%m-%d\")} to {train.index[-1].strftime(\"%Y-%m-%d\")} ({len(train):,} obs, {train_pct*100:.0f}%)')\n",
    "    print(f'Val:   {val.index[0].strftime(\"%Y-%m-%d\")} to {val.index[-1].strftime(\"%Y-%m-%d\")} ({len(val):,} obs, {val_pct*100:.0f}%)')\n",
    "    print(f'Test:  {test.index[0].strftime(\"%Y-%m-%d\")} to {test.index[-1].strftime(\"%Y-%m-%d\")} ({len(test):,} obs, {(1-train_pct-val_pct)*100:.0f}%)')\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d8a6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, filename='data/financial_dataset.csv', label=''):\n",
    "    \"\"\"Save the processed dataset\"\"\"\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    header = f' FINAL DATA SUMMARY - {label}' if label else ' FINAL DATA SUMMARY'\n",
    "    print('\\n' + '='*70)\n",
    "    print(header)\n",
    "    print('='*70)\n",
    "    print(f'\\nShape: {df.shape}')\n",
    "    print(f'Date range: {df.index[0]} to {df.index[-1]}')\n",
    "    print(f'Years covered: {(df.index[-1] - df.index[0]).days / 365.25:.1f}')\n",
    "    print(f'\\nColumns: {list(df.columns)}')\n",
    "    print(f'\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB')\n",
    "    print(f'\\nTarget variable (SP500_Returns) statistics:')\n",
    "    print(f'  Mean: {df[\"SP500_Returns\"].mean():.4f}%')\n",
    "    print(f'  Std: {df[\"SP500_Returns\"].std():.4f}%')\n",
    "    print(f'  Min: {df[\"SP500_Returns\"].min():.4f}%')\n",
    "    print(f'  Max: {df[\"SP500_Returns\"].max():.4f}%')\n",
    "    print(f'  Skewness: {df[\"SP500_Returns\"].skew():.4f}')\n",
    "    print(f'  Kurtosis: {df[\"SP500_Returns\"].kurtosis():.4f}')\n",
    "    print(f'\\nMissing values: {df.isnull().sum().sum()}')\n",
    "\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print('\\nWARNING - Missing values by column:')\n",
    "        for col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                print(f'  {col}: {missing}')\n",
    "\n",
    "    print(f\"\\nSaved dataset to '{filename}'\")\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8eb317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINANCIAL DATA COLLECTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Collecting S&P 500 Market Data\n",
      "----------------------------------------------------------------------\n",
      "   Collected 9007 days of S&P 500 data\n",
      "   Date range: 1990-01-02 00:00:00 to 2025-10-06 00:00:00\n",
      "\n",
      "Collecting FRED Macroeconomic Data\n",
      "----------------------------------------------------------------------\n",
      "   ✓ VIX: 9329 observations\n",
      "   ✓ Treasury_10Y: 16634 observations\n",
      "   ✓ Yield_Spread: 12875 observations\n",
      "   ✓ CPI: 944 observations\n",
      "   ✓ Unemployment: 932 observations\n",
      "   ✓ Fed_Rate: 855 observations\n",
      "   ✓ Consumer_Sentiment: 874 observations\n",
      "   ✓ Industrial_Production: 1280 observations\n",
      "\n",
      "   Summary: Successfully collected 8/8 series\n",
      "\n",
      "Collecting Yahoo Finance Data\n",
      "----------------------------------------------------------------------\n",
      "   ✓ Wilshire5000 (^W5000): 9000 observations\n",
      "\n",
      "   Summary: Successfully collected 1/1 tickers\n",
      "\n",
      "Aligning and combining data...\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Data Quality Check\n",
      "----------------------------------------------------------------------\n",
      "Total missing values: 150\n",
      "\n",
      "Missing by column:\n",
      "  SP500_Returns: 1 (0.0%)\n",
      "  VIX: 3 (0.0%)\n",
      "  Treasury_10Y: 73 (0.8%)\n",
      "  Yield_Spread: 73 (0.8%)\n",
      "\n",
      "Extreme daily returns (>10%): 3\n",
      "Dates with extreme returns:\n",
      "  2008-10-13: 11.58%\n",
      "  2008-10-28: 10.79%\n",
      "  2020-03-16: -11.98%\n",
      "\n",
      "Data types:\n",
      "SP500_Close              float64\n",
      "SP500_Volume               int64\n",
      "SP500_Returns            float64\n",
      "VIX                      float64\n",
      "Treasury_10Y             float64\n",
      "Yield_Spread             float64\n",
      "CPI                      float64\n",
      "Unemployment             float64\n",
      "Fed_Rate                 float64\n",
      "Consumer_Sentiment       float64\n",
      "Industrial_Production    float64\n",
      "Wilshire5000             float64\n",
      "dtype: object\n",
      "\n",
      "Forward-filling macro variables...\n",
      "----------------------------------------------------------------------\n",
      "   ✓ All macro variables forward-filled successfully\n",
      "\n",
      "Applying fixed-shift release date alignment\n",
      "----------------------------------------------------------------------\n",
      "   ✓ Release date adjustments applied\n",
      "   ✓ Final dataset shape: (8918, 14)\n",
      "   ✓ Date range: 1990-01-22 00:00:00 to 2025-10-06 00:00:00\n",
      "\n",
      "======================================================================\n",
      " FINAL DATA SUMMARY - DAILY\n",
      "======================================================================\n",
      "\n",
      "Shape: (8918, 14)\n",
      "Date range: 1990-01-22 00:00:00 to 2025-10-06 00:00:00\n",
      "Years covered: 35.7\n",
      "\n",
      "Columns: ['SP500_Close', 'SP500_Volume', 'SP500_Returns', 'VIX', 'Treasury_10Y', 'Yield_Spread', 'CPI', 'Unemployment', 'Fed_Rate', 'Consumer_Sentiment', 'Industrial_Production', 'Wilshire5000', 'Inflation_YoY', 'SP500_Volatility']\n",
      "\n",
      "Memory usage: 1045.1 KB\n",
      "\n",
      "Target variable (SP500_Returns) statistics:\n",
      "  Mean: 0.0378%\n",
      "  Std: 1.1345%\n",
      "  Min: -11.9841%\n",
      "  Max: 10.7890%\n",
      "  Skewness: -0.2655\n",
      "  Kurtosis: 9.8354\n",
      "\n",
      "Missing values: 271\n",
      "\n",
      "WARNING - Missing values by column:\n",
      "  Inflation_YoY: 252\n",
      "  SP500_Volatility: 19\n",
      "\n",
      "Saved dataset to 'data/financial_dataset_daily.csv'\n",
      "\n",
      "\n",
      "Resampling to ME frequency...\n",
      "----------------------------------------------------------------------\n",
      "   ✓ Resampled shape: (418, 14)\n",
      "   ✓ Date range: 1991-01-31 00:00:00 to 2025-10-31 00:00:00\n",
      "\n",
      "======================================================================\n",
      " FINAL DATA SUMMARY - MONTHLY\n",
      "======================================================================\n",
      "\n",
      "Shape: (418, 14)\n",
      "Date range: 1991-01-31 00:00:00 to 2025-10-31 00:00:00\n",
      "Years covered: 34.7\n",
      "\n",
      "Columns: ['SP500_Close', 'SP500_Volume', 'SP500_Returns', 'VIX', 'Treasury_10Y', 'Yield_Spread', 'CPI', 'Unemployment', 'Fed_Rate', 'Consumer_Sentiment', 'Industrial_Production', 'Wilshire5000', 'Inflation_YoY', 'SP500_Volatility']\n",
      "\n",
      "Memory usage: 49.0 KB\n",
      "\n",
      "Target variable (SP500_Returns) statistics:\n",
      "  Mean: 0.8143%\n",
      "  Std: 4.2270%\n",
      "  Min: -16.9425%\n",
      "  Max: 12.6844%\n",
      "  Skewness: -0.5716\n",
      "  Kurtosis: 1.1004\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "Saved dataset to 'data/financial_dataset_monthly.csv'\n",
      "\n",
      "Data processing done and saved.\n"
     ]
    }
   ],
   "source": [
    "print_section('FINANCIAL DATA COLLECTION PIPELINE')\n",
    "\n",
    "try:\n",
    "    # Collect all data\n",
    "    market_data = collect_market_data()\n",
    "    fred_data = collect_fred_data(FRED_API_KEY)\n",
    "    yahoo_data = collect_yahoo_data(YAHOO_TICKERS)\n",
    "\n",
    "    # Combine and process\n",
    "    combined_data = align_and_combine_data(market_data, fred_data, yahoo_data)\n",
    "    final_data = preprocess_data(combined_data)\n",
    "\n",
    "    # save daily version (raw)\n",
    "    daily_filename = save_data(final_data, 'data/financial_dataset_daily.csv', 'DAILY')\n",
    "    \n",
    "    # create + save monthly version\n",
    "    monthly_data = resample_to_frequency(final_data, freq='ME')\n",
    "    monthly_filename = save_data(monthly_data, 'data/financial_dataset_monthly.csv', 'MONTHLY')\n",
    "\n",
    "    print(\"\\nData processing done and saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during data collection: {str(e)}\")\n",
    "    print(\"Please check your FRED API key and internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f03bb5-3af7-4b25-83a2-963c71d36e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating visualizations...\n",
      "   Saved visualization: data_overview.png\n"
     ]
    }
   ],
   "source": [
    "print('\\n Creating visualizations...')\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# 1. returns over time\n",
    "axes[0].plot(final_data.index, final_data['SP500_Returns'], alpha=0.7, linewidth=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('S&P 500 Daily Returns Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Return (%)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. distribution\n",
    "axes[1].hist(final_data['SP500_Returns'].dropna(), bins=100, \n",
    "             edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Distribution of Daily Returns', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Return (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. cumulative returns\n",
    "cumulative = (1 + final_data['SP500_Returns']/100).cumprod()\n",
    "axes[2].plot(final_data.index, cumulative, linewidth=1.5)\n",
    "axes[2].set_title('Cumulative Returns (Growth of $1)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Value ($)')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/data_overview.png', dpi=150, bbox_inches='tight')\n",
    "print('   Saved visualization: data_overview.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de2b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e5070c8",
   "metadata": {},
   "source": [
    "## Prepare Optimized data with Gold features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9609a8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0438a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(series, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gold_data_optimized(start_date, end_date):\n",
    "    \"\"\"Collect gold data with better error handling\"\"\"\n",
    "    print(\"Collecting gold price data (optimized approach)...\")\n",
    "\n",
    "    try:\n",
    "        # Try gold futures first\n",
    "        print(\"   Attempting gold futures (GC=F)...\")\n",
    "        gold_data = yf.download('GC=F', \n",
    "                               start=start_date - pd.Timedelta(days=60), \n",
    "                               end=end_date + pd.Timedelta(days=1), \n",
    "                               progress=False)\n",
    "\n",
    "        if hasattr(gold_data.columns, 'nlevels') and gold_data.columns.nlevels > 1:\n",
    "            gold_data.columns = [col[0] for col in gold_data.columns]\n",
    "\n",
    "        if len(gold_data) > 100:  # Reasonable amount of data\n",
    "            print(f\"   Gold futures data: {len(gold_data)} observations\")\n",
    "            return gold_data['Close']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Gold futures failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Try SPDR Gold Trust ETF (GLD)\n",
    "        print(\"   Attempting Gold ETF (GLD)...\")\n",
    "        gld_data = yf.download('GLD', \n",
    "                              start=start_date - pd.Timedelta(days=60), \n",
    "                              end=end_date + pd.Timedelta(days=1),\n",
    "                              progress=False)\n",
    "\n",
    "        if hasattr(gld_data.columns, 'nlevels') and gld_data.columns.nlevels > 1:\n",
    "            gld_data.columns = [col[0] for col in gld_data.columns]\n",
    "\n",
    "        if len(gld_data) > 100:\n",
    "            # Convert GLD to approximate gold price (GLD ≈ gold/10)\n",
    "            gold_proxy = gld_data['Close'] * 10\n",
    "            print(f\"   Gold ETF data: {len(gld_data)} observations\")\n",
    "            return gold_proxy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Gold ETF failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Try iShares Gold Trust (IAU) \n",
    "        print(\"   Attempting Gold ETF (IAU)...\")\n",
    "        iau_data = yf.download('IAU', \n",
    "                              start=start_date - pd.Timedelta(days=60), \n",
    "                              end=end_date + pd.Timedelta(days=1),\n",
    "                              progress=False)\n",
    "\n",
    "        if hasattr(iau_data.columns, 'nlevels') and iau_data.columns.nlevels > 1:\n",
    "            iau_data.columns = [col[0] for col in iau_data.columns]\n",
    "\n",
    "        if len(iau_data) > 100:\n",
    "            # Convert IAU to approximate gold price (IAU ≈ gold/100)\n",
    "            gold_proxy = iau_data['Close'] * 100\n",
    "            print(f\"   Alternative Gold ETF data: {len(iau_data)} observations\")\n",
    "            return gold_proxy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Alternative Gold ETF failed: {e}\")\n",
    "\n",
    "    print(\"BIG PROBLEM. GOLD DATA DOES NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50b614d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_optimized_gold_features(df):\n",
    "    \"\"\"\n",
    "    Add ONLY the most valuable gold features that don't duplicate existing signals\n",
    "\n",
    "    Selection criteria:\n",
    "    1. Unique signal not captured by VIX/technical indicators\n",
    "    2. Proven predictive value in research\n",
    "    3. Low correlation with existing features\n",
    "    4. Captures different market dynamics (monetary policy, crisis, etc.)\n",
    "    \"\"\"\n",
    "    print(\" Adding optimized gold features (quality over quantity)...\")\n",
    "\n",
    "    # Get gold data\n",
    "    start_date = df.index[0]\n",
    "    end_date = df.index[-1]\n",
    "    \n",
    "    gold_prices = collect_gold_data_optimized(start_date, end_date)\n",
    "\n",
    "    # Align with stock data\n",
    "    gold_prices_aligned = gold_prices.reindex(df.index, method='ffill')\n",
    "\n",
    "    # Core gold features - these provide unique signals\n",
    "    df['Gold_Price'] = gold_prices_aligned\n",
    "    df['Gold_Returns'] = gold_prices_aligned.pct_change() * 100\n",
    "\n",
    "    print(\"   ✓ Basic gold price and returns\")\n",
    "\n",
    "    # 1. GOLD/SPX RATIO - Key risk-on/risk-off indicator (UNIQUE SIGNAL)\n",
    "    if 'SP500_Close' in df.columns:\n",
    "        df['Gold_SPX_Ratio'] = df['Gold_Price'] / df['SP500_Close']\n",
    "        # Normalized version to capture deviations\n",
    "        df['Gold_SPX_Ratio_Norm'] = ((df['Gold_SPX_Ratio'] - df['Gold_SPX_Ratio'].rolling(60).mean()) / \n",
    "                                     df['Gold_SPX_Ratio'].rolling(60).std())\n",
    "        print(\"   ✓ Gold/SPX ratio (primary risk sentiment indicator)\")\n",
    "\n",
    "    # 2. GOLD vs REAL INTEREST RATES - Monetary policy impact (UNIQUE SIGNAL)\n",
    "    if 'Treasury_10Y' in df.columns:\n",
    "        # Approximate real rates using Treasury yield minus rolling inflation proxy\n",
    "        if 'CPI' in df.columns:\n",
    "            inflation_proxy = df['CPI'].pct_change(252) * 100  # YoY change\n",
    "            df['Real_Interest_Rate'] = df['Treasury_10Y'] - inflation_proxy\n",
    "            df['Gold_Real_Rate_Signal'] = (df['Real_Interest_Rate'] < 0).astype(int)\n",
    "            print(\"   ✓ Gold vs real interest rates (monetary policy signal)\")\n",
    "        else:\n",
    "            # Simplified version without CPI\n",
    "            df['Gold_Yield_Inverse'] = 1 / (df['Treasury_10Y'] + 0.01)  # Avoid division by zero\n",
    "            print(\"   ✓ Gold vs nominal rates (simplified monetary signal)\")\n",
    "\n",
    "    # 3. GOLD MOMENTUM - Different timeframe than stock momentum (COMPLEMENTARY SIGNAL)\n",
    "    df['Gold_Momentum_10d'] = df['Gold_Price'].pct_change(10)\n",
    "    df['Gold_Momentum_Strength'] = (df['Gold_Momentum_10d'] > df['Gold_Momentum_10d'].rolling(60).quantile(0.7)).astype(int)\n",
    "    print(\"   ✓ Gold momentum (10-day, different from stock momentum)\")\n",
    "\n",
    "    # 4. GOLD VOLATILITY REGIME - Crisis detection (UNIQUE SIGNAL)\n",
    "    gold_vol = df['Gold_Returns'].rolling(20).std()\n",
    "    df['Gold_Vol_Regime'] = (gold_vol > gold_vol.rolling(120).quantile(0.8)).astype(int)\n",
    "    print(\"   ✓ Gold volatility regime (crisis detection)\")\n",
    "\n",
    "    # 5. GOLD TREND PERSISTENCE - Long-term trend strength (UNIQUE SIGNAL)\n",
    "    gold_ma_short = df['Gold_Price'].rolling(20).mean()\n",
    "    gold_ma_long = df['Gold_Price'].rolling(60).mean()\n",
    "    df['Gold_Trend_Direction'] = (gold_ma_short > gold_ma_long).astype(int)\n",
    "    df['Gold_Trend_Strength'] = (df['Gold_Price'] - gold_ma_long) / gold_ma_long\n",
    "    print(\"   ✓ Gold trend persistence (20d vs 60d MA)\")\n",
    "\n",
    "    # 6. GOLD SAFE HAVEN ACTIVATION - Crisis correlation (UNIQUE SIGNAL)\n",
    "    # When gold rises while stocks fall (safe haven behavior)\n",
    "    if 'SP500_Returns' in df.columns:\n",
    "        df['Gold_Safe_Haven'] = (\n",
    "            (df['Gold_Returns'] > 0) & \n",
    "            (df['SP500_Returns'] < -1.0)  # Significant stock decline\n",
    "        ).astype(int)\n",
    "        print(\"   ✓ Gold safe haven activation (crisis behavior)\")\n",
    "\n",
    "    # 7. SELECTIVE GOLD LAGS - Only the most predictive (MEMORY SIGNAL)\n",
    "    # Research shows 5-day lag is often most predictive for gold\n",
    "    df['Gold_Returns_lag5'] = df['Gold_Returns'].shift(5)\n",
    "    print(\"   ✓ Gold 5-day lag (momentum persistence)\")\n",
    "\n",
    "    # Count added gold features\n",
    "    gold_features = [col for col in df.columns if 'Gold' in col or 'Real_Interest_Rate' in col]\n",
    "    print(f\"   Total optimized gold features: {len(gold_features)}\")\n",
    "\n",
    "    # Display the selected features\n",
    "    print(\"\\n Selected Gold Features:\")\n",
    "    for i, feature in enumerate(gold_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cc0cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_core_technical_features(df):\n",
    "    print(\"Creating core technical features (53-feature winning set)...\")\n",
    "\n",
    "    df_enhanced = df.copy()\n",
    "\n",
    "    # 1. VIX features (fear/volatility indicators)\n",
    "    if 'VIX' in df_enhanced.columns:\n",
    "        for lag in [1, 5, 10]:\n",
    "            df_enhanced[f'VIX_lag_{lag}'] = df_enhanced['VIX'].shift(lag)\n",
    "\n",
    "        df_enhanced['VIX_MA_10'] = df_enhanced['VIX'].rolling(10).mean()\n",
    "        df_enhanced['VIX_relative'] = df_enhanced['VIX'] / df_enhanced['VIX_MA_10']\n",
    "        df_enhanced['VIX_spike'] = (df_enhanced['VIX'] > df_enhanced['VIX_MA_10'] * 1.5).astype(int)\n",
    "        print(\"   ✓ VIX features (6 features)\")\n",
    "\n",
    "    # 2. Rolling volatility features\n",
    "    if 'SP500_Returns' in df_enhanced.columns:\n",
    "        for window in [5, 10, 20]:\n",
    "            df_enhanced[f'Volatility_{window}d'] = df_enhanced['SP500_Returns'].rolling(window).std()\n",
    "        print(\"   ✓ Rolling volatility features (3 features)\")\n",
    "\n",
    "    # 3. Moving average features\n",
    "    if 'SP500_Close' in df_enhanced.columns:\n",
    "        for ma_period in [10, 20, 50]:\n",
    "            df_enhanced[f'MA_{ma_period}'] = df_enhanced['SP500_Close'].rolling(ma_period).mean()\n",
    "            df_enhanced[f'Price_to_MA_{ma_period}'] = df_enhanced['SP500_Close'] / df_enhanced[f'MA_{ma_period}']\n",
    "\n",
    "        # MA crossover signals\n",
    "        df_enhanced['MA_10_vs_20'] = (df_enhanced['MA_10'] > df_enhanced['MA_20']).astype(int)\n",
    "        df_enhanced['MA_20_vs_50'] = (df_enhanced['MA_20'] > df_enhanced['MA_50']).astype(int)\n",
    "        print(\"   ✓ Moving average features (8 features)\")\n",
    "\n",
    "    # 4. RSI and momentum oscillators\n",
    "    if 'SP500_Close' in df_enhanced.columns:\n",
    "        df_enhanced['RSI'] = calculate_rsi(df_enhanced['SP500_Close'])\n",
    "        df_enhanced['RSI_overbought'] = (df_enhanced['RSI'] > 70).astype(int)\n",
    "        df_enhanced['RSI_oversold'] = (df_enhanced['RSI'] < 30).astype(int)\n",
    "        print(\"   ✓ RSI features (3 features)\")\n",
    "\n",
    "    # 5. Momentum features\n",
    "    if 'SP500_Close' in df_enhanced.columns:\n",
    "        for period in [5, 10, 20]:\n",
    "            df_enhanced[f'Momentum_{period}d'] = df_enhanced['SP500_Close'].pct_change(period)\n",
    "            df_enhanced[f'Momentum_{period}d_positive'] = (df_enhanced[f'Momentum_{period}d'] > 0).astype(int)\n",
    "        print(\"   ✓ Momentum features (6 features)\")\n",
    "\n",
    "    # 6. Time-based features\n",
    "    df_enhanced['DayOfWeek'] = df_enhanced.index.dayofweek\n",
    "    df_enhanced['Month'] = df_enhanced.index.month\n",
    "    df_enhanced['Quarter'] = df_enhanced.index.quarter\n",
    "    df_enhanced['IsMonthEnd'] = df_enhanced.index.is_month_end.astype(int)\n",
    "    print(\"   ✓ Time features (4 features)\")\n",
    "\n",
    "    # 7. Yield curve features\n",
    "    if 'Yield_Spread' in df_enhanced.columns:\n",
    "        df_enhanced['Yield_Spread_MA'] = df_enhanced['Yield_Spread'].rolling(20).mean()\n",
    "        df_enhanced['Yield_Spread_relative'] = df_enhanced['Yield_Spread'] / df_enhanced['Yield_Spread_MA']\n",
    "        df_enhanced['Yield_Curve_Inversion'] = (df_enhanced['Yield_Spread'] < 0).astype(int)\n",
    "        print(\"   ✓ Yield curve features (3 features)\")\n",
    "\n",
    "    # 8. Treasury features\n",
    "    if 'Treasury_10Y' in df_enhanced.columns:\n",
    "        df_enhanced['Treasury_10Y_change'] = df_enhanced['Treasury_10Y'].diff()\n",
    "        df_enhanced['Treasury_10Y_MA'] = df_enhanced['Treasury_10Y'].rolling(20).mean()\n",
    "        df_enhanced['Treasury_Rising'] = (df_enhanced['Treasury_10Y_change'] > 0).astype(int)\n",
    "        print(\"   ✓ Treasury features (3 features)\")\n",
    "\n",
    "    # 9. CPI features\n",
    "    if 'CPI' in df_enhanced.columns:\n",
    "        df_enhanced['CPI_YoY'] = df_enhanced['CPI'].pct_change(252)\n",
    "        df_enhanced['CPI_acceleration'] = df_enhanced['CPI_YoY'].diff()\n",
    "        print(\"   ✓ CPI features (2 features)\")\n",
    "\n",
    "    # 10. Unemployment features\n",
    "    if 'Unemployment' in df_enhanced.columns:\n",
    "        df_enhanced['Unemployment_change'] = df_enhanced['Unemployment'].diff()\n",
    "        df_enhanced['Unemployment_Rising'] = (df_enhanced['Unemployment_change'] > 0).astype(int)\n",
    "        print(\"   ✓ Unemployment features (2 features)\")\n",
    "\n",
    "    # 11. Cross-asset correlations\n",
    "    if 'Treasury_10Y' in df_enhanced.columns and 'SP500_Returns' in df_enhanced.columns:\n",
    "        df_enhanced['Stock_Bond_Corr'] = df_enhanced['SP500_Returns'].rolling(60).corr(\n",
    "            df_enhanced['Treasury_10Y'].diff()\n",
    "        )\n",
    "        print(\"   ✓ Cross-asset correlation (1 feature)\")\n",
    "\n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba26095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimized_lstm_data():\n",
    "    \"\"\"Main data preparation with optimized gold integration\"\"\"\n",
    "    print(\" OPTIMIZED Data Preparation for LSTM\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    filename = 'data/financial_dataset_daily.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "        print(f\"Loaded {filename}: {df.shape}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"FileNotFoundError\")\n",
    "\n",
    "    # Create core features (the winning 53-feature set)\n",
    "    df_core = create_core_technical_features(df)\n",
    "\n",
    "    # Add optimized gold features\n",
    "    df_optimized = add_optimized_gold_features(df_core)\n",
    "\n",
    "    # Clean data\n",
    "    df_optimized = df_optimized.dropna()\n",
    "\n",
    "    # Final feature count\n",
    "    original_features = df.shape[1]\n",
    "    optimized_features = df_optimized.shape[1]\n",
    "    gold_features = len([col for col in df_optimized.columns if 'Gold' in col or 'Real_Interest_Rate' in col])\n",
    "    core_features = optimized_features - gold_features\n",
    "\n",
    "    print(f\"\\n OPTIMIZED DATA PREPARATION COMPLETE!\")\n",
    "    print(\"=\"*45)\n",
    "    print(f\"Original features: {original_features}\")\n",
    "    print(f\"Core features: {core_features}\")\n",
    "    print(f\"Optimized gold features: {gold_features}\")\n",
    "    print(f\"Total optimized features: {optimized_features}\")\n",
    "    print(f\"Final observations: {len(df_optimized)}\")\n",
    "\n",
    "    # Feature breakdown\n",
    "    print(f\"\\n FEATURE BREAKDOWN:\")\n",
    "    feature_categories = {\n",
    "        'Market Data': [col for col in df_optimized.columns if 'SP500' in col],\n",
    "        'VIX Features': [col for col in df_optimized.columns if 'VIX' in col],\n",
    "        'Technical Indicators': [col for col in df_optimized.columns if any(x in col for x in ['MA_', 'RSI', 'Momentum']) and 'Gold' not in col],\n",
    "        'Volatility': [col for col in df_optimized.columns if 'Volatility' in col],\n",
    "        'Time Features': [col for col in df_optimized.columns if any(x in col for x in ['DayOfWeek', 'Month', 'Quarter'])],\n",
    "        'Macro Economic': [col for col in df_optimized.columns if any(x in col for x in ['Treasury', 'Yield', 'CPI', 'Unemployment']) and 'Gold' not in col],\n",
    "        'Optimized Gold': [col for col in df_optimized.columns if 'Gold' in col or 'Real_Interest_Rate' in col]\n",
    "    }\n",
    "\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"\\n{category} ({len(features)} features):\")\n",
    "            for feature in features:\n",
    "                print(f\"   • {feature}\")\n",
    "\n",
    "    # Save optimized dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'data/optimized_financial_data_{timestamp}.csv'\n",
    "    df_optimized.to_csv(filename)\n",
    "\n",
    "    # Standard filename for easy loading\n",
    "    df_optimized.to_csv('data/optimized_financial_data.csv')\n",
    "\n",
    "    print(f\"\\n Optimized data saved:\")\n",
    "    print(f\"   Main file: optimized_financial_data.csv\")\n",
    "    print(f\"   Timestamped: {filename}\")\n",
    "\n",
    "    return df_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2ab726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OPTIMIZED Data Preparation for LSTM\n",
      "=============================================\n",
      "Loaded data/financial_dataset_daily.csv: (8918, 14)\n",
      "Creating core technical features (53-feature winning set)...\n",
      "   ✓ VIX features (6 features)\n",
      "   ✓ Rolling volatility features (3 features)\n",
      "   ✓ Moving average features (8 features)\n",
      "   ✓ RSI features (3 features)\n",
      "   ✓ Momentum features (6 features)\n",
      "   ✓ Time features (4 features)\n",
      "   ✓ Yield curve features (3 features)\n",
      "   ✓ Treasury features (3 features)\n",
      "   ✓ CPI features (2 features)\n",
      "   ✓ Unemployment features (2 features)\n",
      "   ✓ Cross-asset correlation (1 feature)\n",
      " Adding optimized gold features (quality over quantity)...\n",
      "Collecting gold price data (optimized approach)...\n",
      "   Attempting gold futures (GC=F)...\n",
      "   Gold futures data: 6298 observations\n",
      "   ✓ Basic gold price and returns\n",
      "   ✓ Gold/SPX ratio (primary risk sentiment indicator)\n",
      "   ✓ Gold vs real interest rates (monetary policy signal)\n",
      "   ✓ Gold momentum (10-day, different from stock momentum)\n",
      "   ✓ Gold volatility regime (crisis detection)\n",
      "   ✓ Gold trend persistence (20d vs 60d MA)\n",
      "   ✓ Gold safe haven activation (crisis behavior)\n",
      "   ✓ Gold 5-day lag (momentum persistence)\n",
      "   Total optimized gold features: 13\n",
      "\n",
      " Selected Gold Features:\n",
      "    1. Gold_Price\n",
      "    2. Gold_Returns\n",
      "    3. Gold_SPX_Ratio\n",
      "    4. Gold_SPX_Ratio_Norm\n",
      "    5. Real_Interest_Rate\n",
      "    6. Gold_Real_Rate_Signal\n",
      "    7. Gold_Momentum_10d\n",
      "    8. Gold_Momentum_Strength\n",
      "    9. Gold_Vol_Regime\n",
      "   10. Gold_Trend_Direction\n",
      "   11. Gold_Trend_Strength\n",
      "   12. Gold_Safe_Haven\n",
      "   13. Gold_Returns_lag5\n",
      "\n",
      " OPTIMIZED DATA PREPARATION COMPLETE!\n",
      "=============================================\n",
      "Original features: 14\n",
      "Core features: 55\n",
      "Optimized gold features: 13\n",
      "Total optimized features: 68\n",
      "Final observations: 6207\n",
      "\n",
      " FEATURE BREAKDOWN:\n",
      "\n",
      "Market Data (4 features):\n",
      "   • SP500_Close\n",
      "   • SP500_Volume\n",
      "   • SP500_Returns\n",
      "   • SP500_Volatility\n",
      "\n",
      "VIX Features (7 features):\n",
      "   • VIX\n",
      "   • VIX_lag_1\n",
      "   • VIX_lag_5\n",
      "   • VIX_lag_10\n",
      "   • VIX_MA_10\n",
      "   • VIX_relative\n",
      "   • VIX_spike\n",
      "\n",
      "Technical Indicators (18 features):\n",
      "   • VIX_MA_10\n",
      "   • MA_10\n",
      "   • Price_to_MA_10\n",
      "   • MA_20\n",
      "   • Price_to_MA_20\n",
      "   • MA_50\n",
      "   • Price_to_MA_50\n",
      "   • MA_10_vs_20\n",
      "   • MA_20_vs_50\n",
      "   • RSI\n",
      "   • RSI_overbought\n",
      "   • RSI_oversold\n",
      "   • Momentum_5d\n",
      "   • Momentum_5d_positive\n",
      "   • Momentum_10d\n",
      "   • Momentum_10d_positive\n",
      "   • Momentum_20d\n",
      "   • Momentum_20d_positive\n",
      "\n",
      "Volatility (4 features):\n",
      "   • SP500_Volatility\n",
      "   • Volatility_5d\n",
      "   • Volatility_10d\n",
      "   • Volatility_20d\n",
      "\n",
      "Time Features (4 features):\n",
      "   • DayOfWeek\n",
      "   • Month\n",
      "   • Quarter\n",
      "   • IsMonthEnd\n",
      "\n",
      "Macro Economic (14 features):\n",
      "   • Treasury_10Y\n",
      "   • Yield_Spread\n",
      "   • CPI\n",
      "   • Unemployment\n",
      "   • Yield_Spread_MA\n",
      "   • Yield_Spread_relative\n",
      "   • Yield_Curve_Inversion\n",
      "   • Treasury_10Y_change\n",
      "   • Treasury_10Y_MA\n",
      "   • Treasury_Rising\n",
      "   • CPI_YoY\n",
      "   • CPI_acceleration\n",
      "   • Unemployment_change\n",
      "   • Unemployment_Rising\n",
      "\n",
      "Optimized Gold (13 features):\n",
      "   • Gold_Price\n",
      "   • Gold_Returns\n",
      "   • Gold_SPX_Ratio\n",
      "   • Gold_SPX_Ratio_Norm\n",
      "   • Real_Interest_Rate\n",
      "   • Gold_Real_Rate_Signal\n",
      "   • Gold_Momentum_10d\n",
      "   • Gold_Momentum_Strength\n",
      "   • Gold_Vol_Regime\n",
      "   • Gold_Trend_Direction\n",
      "   • Gold_Trend_Strength\n",
      "   • Gold_Safe_Haven\n",
      "   • Gold_Returns_lag5\n",
      "\n",
      " Optimized data saved:\n",
      "   Main file: optimized_financial_data.csv\n",
      "   Timestamped: data/optimized_financial_data_20251006_200327.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_optimized = prepare_optimized_lstm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbb3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a830a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
